{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae1f50ec",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_function_call_currency_calculator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a71fa36",
   "metadata": {},
   "source": [
    "# Websockets: streaming with websockets\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install `pyautogen`:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b803c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"pyautogen>=0.2.3\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ebd2397",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dca301a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "from websockets.sync.client import connect as ws_connect\n",
    "from autogen.io.websockets import IOWebsockets\n",
    "\n",
    "import autogen\n",
    "from autogen.cache import Cache\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92fde41f",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the models with matching names are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2024-02-15-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo-16k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2024-02-15-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "You can set the value of config_list in any way you prefer. Please refer to this [notebook](https://github.com/microsoft/autogen/blob/main/website/docs/llm_configuration.ipynb) for full code examples of the different methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b9526e7",
   "metadata": {},
   "source": [
    "## Defining `on_connect` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fb85afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_connect(iostream: IOWebsockets) -> None:\n",
    "    try:\n",
    "        print(f\" - on_connect(): Connected to client using IOWebsockets {iostream}\", flush=True)\n",
    "\n",
    "        print(\" - on_connect(): Receiving message from client.\", flush=True)\n",
    "\n",
    "        initial_msg = iostream.input()\n",
    "\n",
    "        llm_config = {\n",
    "            \"config_list\": config_list,\n",
    "            \"stream\": True,\n",
    "        }\n",
    "\n",
    "        agent = autogen.ConversableAgent(\n",
    "            name=\"chatbot\",\n",
    "            system_message=\"Complete a task given to you and reply TERMINATE when the task is done.\",\n",
    "            llm_config=llm_config,\n",
    "            iostream=iostream,\n",
    "        )\n",
    "\n",
    "        # create a UserProxyAgent instance named \"user_proxy\"\n",
    "        user_proxy = autogen.UserProxyAgent(\n",
    "            name=\"user_proxy\",\n",
    "            system_message=\"A proxy for the user.\",\n",
    "            is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "            human_input_mode=\"NEVER\",\n",
    "            max_consecutive_auto_reply=10,\n",
    "            code_execution_config=False,\n",
    "            iostream=iostream,\n",
    "        )\n",
    "\n",
    "        # we will use a temporary directory as the cache path root to ensure fresh completion each time\n",
    "        with TemporaryDirectory() as cache_path_root:\n",
    "            with Cache.disk(cache_path_root=cache_path_root) as cache:\n",
    "                print(\n",
    "                    f\" - on_connect(): Initiating chat with agent {agent} using message '{initial_msg}'\",\n",
    "                    flush=True,\n",
    "                )\n",
    "                user_proxy.initiate_chat(  # noqa: F704\n",
    "                    agent,\n",
    "                    message=initial_msg,\n",
    "                    cache=cache,\n",
    "                )\n",
    "    except Exception as e:\n",
    "        print(f\" - on_connect(): Exception occurred: {e}\", flush=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef868a",
   "metadata": {},
   "source": [
    "## Run websockets server in a thread and connect to it with websockets client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fbe004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - test_setup() with websocket server running on ws://127.0.0.1:8765.\n",
      " - _handler(): Client connected on <websockets.sync.server.ServerConnection object at 0x7fb1dea3f520>\n",
      " - on_connect(): Connected to client using IOWebsockets <autogen.io.websockets.IOWebsockets object at 0x7fb1dea3eda0>\n",
      " - Connected to server on ws://127.0.0.1:8765\n",
      " - on_connect(): Receiving message from client.\n",
      " - Sending message to server.\n",
      " - on_connect(): Initiating chat with agent <autogen.agentchat.conversable_agent.ConversableAgent object at 0x7fb1dea3ed10> using message 'Please write a poem about spring in a city of your choice.'\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\u001b[32mPlease write a poem about spring in a city of your choice.\n",
      "--------------------------------------------------------------------------------\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0mIn Paris, blooms a spring delight,\n",
      "Along the Seine, the petals bright.\n",
      "Café chairs spill to cobbled street,\n",
      "As winter's chill makes its retreat.\n",
      "\n",
      "The Eiffel stands, a silent guard,\n",
      "Above the blossoms in the yard.\n",
      "The jardins wake from their long sleep,\n",
      "In colors vivid, lush, and deep.\n",
      "\n",
      "Lovers stroll on Montmartre's crest,\n",
      "While springtime's warmth fills every breast.\n",
      "The artists paint with fervor new,\n",
      "As sky above turns bluest blue.\n",
      "\n",
      "Along the Champs-Élysées, trees,\n",
      "Adorn their leaves with softest breeze.\n",
      "The city hums a lively tune,\n",
      "Under the watchful, smiling moon.\n",
      "\n",
      "From Marais to the Latin Quarter,\n",
      "Life reborn by water's border.\n",
      "Pastries rich with fruit and cream,\n",
      "Sit like jewels, a gourmand's dream.\n",
      "\n",
      "In every park and open space,\n",
      "Parisians find a tranquil place.\n",
      "Where flowers dance and children play,\n",
      "In the splendor of the May.\n",
      "\n",
      "So here's to spring in Paris, hearts,\n",
      "Where every ending is a start.\n",
      "A season where the old makes way,\n",
      "And love is born anew each day.\n",
      "\n",
      "TERMINATE\u001b[0m\n",
      "\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "In Paris, blooms a spring delight,\n",
      "Along the Seine, the petals bright.\n",
      "Café chairs spill to cobbled street,\n",
      "As winter's chill makes its retreat.\n",
      "\n",
      "The Eiffel stands, a silent guard,\n",
      "Above the blossoms in the yard.\n",
      "The jardins wake from their long sleep,\n",
      "In colors vivid, lush, and deep.\n",
      "\n",
      "Lovers stroll on Montmartre's crest,\n",
      "While springtime's warmth fills every breast.\n",
      "The artists paint with fervor new,\n",
      "As sky above turns bluest blue.\n",
      "\n",
      "Along the Champs-Élysées, trees,\n",
      "Adorn their leaves with softest breeze.\n",
      "The city hums a lively tune,\n",
      "Under the watchful, smiling moon.\n",
      "\n",
      "From Marais to the Latin Quarter,\n",
      "Life reborn by water's border.\n",
      "Pastries rich with fruit and cream,\n",
      "Sit like jewels, a gourmand's dream.\n",
      "\n",
      "In every park and open space,\n",
      "Parisians find a tranquil place.\n",
      "Where flowers dance and children play,\n",
      "In the splendor of the May.\n",
      "\n",
      "So here's to spring in Paris, hearts,\n",
      "Where every ending is a start.\n",
      "A season where the old makes way,\n",
      "And love is born anew each day.\n",
      "\n",
      "TERMINATE\n",
      " - Received TERMINATE message. Exiting.\n"
     ]
    }
   ],
   "source": [
    "with IOWebsockets.run_server_in_thread(on_connect=on_connect, port=8765) as uri:\n",
    "    print(f\" - test_setup() with websocket server running on {uri}.\", flush=True)\n",
    "\n",
    "    with ws_connect(uri) as websocket:\n",
    "        print(f\" - Connected to server on {uri}\", flush=True)\n",
    "\n",
    "        print(\" - Sending message to server.\", flush=True)\n",
    "        # websocket.send(\"2+2=?\")\n",
    "        websocket.send(\"Please write a poem about spring in a city of your choice.\")\n",
    "\n",
    "        while True:\n",
    "            message = websocket.recv()\n",
    "            message = message.decode(\"utf-8\") if isinstance(message, bytes) else message\n",
    "            # drop the newline character\n",
    "            if message.endswith(\"\\n\"):\n",
    "                message = message[:-1]\n",
    "\n",
    "            print(message, end=\"\", flush=True)\n",
    "\n",
    "            if \"TERMINATE\" in message:\n",
    "                print()\n",
    "                print(\" - Received TERMINATE message. Exiting.\", flush=True)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14306e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
