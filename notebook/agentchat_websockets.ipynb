{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae1f50ec",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_function_call_currency_calculator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a71fa36",
   "metadata": {},
   "source": [
    "# Websockets: streaming with websockets\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install `pyautogen`:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b803c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"pyautogen>=0.2.3\", fastapi, uvicorn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ebd2397",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca301a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "from websockets.sync.client import connect as ws_connect\n",
    "from autogen.io.websockets import IOWebsockets\n",
    "\n",
    "import autogen\n",
    "from autogen.cache import Cache\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92fde41f",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the models with matching names are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2024-02-15-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo-16k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2024-02-15-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "You can set the value of config_list in any way you prefer. Please refer to this [notebook](https://github.com/microsoft/autogen/blob/main/website/docs/llm_configuration.ipynb) for full code examples of the different methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b9526e7",
   "metadata": {},
   "source": [
    "## Defining on_connect function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb85afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_connect(iostream: IOWebsockets) -> None:\n",
    "    try:\n",
    "        print(f\" - on_connect(): Connected to client using IOWebsockets {iostream}\", flush=True)\n",
    "\n",
    "        print(\" - on_connect(): Receiving message from client.\", flush=True)\n",
    "\n",
    "        initial_msg = iostream.input()\n",
    "\n",
    "        llm_config = {\n",
    "            \"config_list\": config_list,\n",
    "            \"stream\": True,\n",
    "        }\n",
    "\n",
    "        agent = autogen.ConversableAgent(\n",
    "            name=\"chatbot\",\n",
    "            system_message=\"Complete a task given to you and reply TERMINATE when the task is done. If asked about the weather, use tool weather_forecast(city) to get the weather forecast for a city.\",\n",
    "            llm_config=llm_config,\n",
    "            iostream=iostream,\n",
    "        )\n",
    "\n",
    "        # create a UserProxyAgent instance named \"user_proxy\"\n",
    "        user_proxy = autogen.UserProxyAgent(\n",
    "            name=\"user_proxy\",\n",
    "            system_message=\"A proxy for the user.\",\n",
    "            is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "            human_input_mode=\"NEVER\",\n",
    "            max_consecutive_auto_reply=10,\n",
    "            code_execution_config=False,\n",
    "            iostream=iostream,\n",
    "        )\n",
    "\n",
    "        @user_proxy.register_for_execution()\n",
    "        @agent.register_for_llm(description=\"Weather forecats for a city\")\n",
    "        def weather_forecast(city: str) -> str:\n",
    "            return f\"The weather forecast for {city} is sunny.\"\n",
    "\n",
    "        # we will use a temporary directory as the cache path root to ensure fresh completion each time\n",
    "        with TemporaryDirectory() as cache_path_root:\n",
    "            with Cache.disk(cache_path_root=cache_path_root) as cache:\n",
    "                print(\n",
    "                    f\" - on_connect(): Initiating chat with agent {agent} using message '{initial_msg}'\",\n",
    "                    flush=True,\n",
    "                )\n",
    "                user_proxy.initiate_chat(  # noqa: F704\n",
    "                    agent,\n",
    "                    message=initial_msg,\n",
    "                    cache=cache,\n",
    "                )\n",
    "    except Exception as e:\n",
    "        print(f\" - on_connect(): Exception occurred: {e}\", flush=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef868a",
   "metadata": {},
   "source": [
    "## Testing websockets server with Python client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbe004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - test_setup() with websocket server running on ws://127.0.0.1:8765.\n",
      " - _handler(): Client connected on <websockets.sync.server.ServerConnection object at 0x7f16a092f0a0>\n",
      " - Connected to server on ws://127.0.0.1:8765\n",
      " - on_connect(): Connected to client using IOWebsockets <autogen.io.websockets.IOWebsockets object at 0x7f16a092ed10>\n",
      " - Sending message to server.\n",
      " - on_connect(): Receiving message from client.\n",
      " - on_connect(): Initiating chat with agent <autogen.agentchat.conversable_agent.ConversableAgent object at 0x7f16a092ddb0> using message 'Check out the weather in Paris and write a poem about it.'\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "Check out the weather in Paris and write a poem about it.\n",
      "--------------------------------------------------------------------------------\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\u001b[32m\u001b[0m\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\u001b[32m***** Suggested tool Call (call_EIVErWgI8qtlt0U3jiWTgAvH): weather_forecast *****\u001b[0mArguments: \n",
      "{\"city\":\"Paris\"}\u001b[32m*********************************************************************************\u001b[0m\n",
      "--------------------------------------------------------------------------------\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION weather_forecast...\u001b[0m\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\u001b[32m***** Response from calling tool \"call_EIVErWgI8qtlt0U3jiWTgAvH\" *****\u001b[0mThe weather forecast for Paris is sunny.\u001b[32m**********************************************************************\u001b[0m\n",
      "--------------------------------------------------------------------------------\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\u001b[32mIn Paris, the city of light and love,  The sun bestows warm rays from above.  Golden beams weave through streets so grand,  Caressing the stone and softening the land.\n",
      "In gardens where lovers often stroll,  Sunshine blankets each meandering knoll.  Reflections dance on the Seine's smooth face,  Gilding the waters with gentle grace.\n",
      "Notre Dame basks in the daylight's kiss,  Its spire cutting the blue, a sight of bliss.  In open cafes, the air tastes sweet,  As pastries warm in the sun's steady beat.\n",
      "The dappled shade of the chestnut trees,  Whispers of history ride on the breeze.  The Eiffel Tower stands tall and proud,  Its shadow on the city, a sun-drawn shroud.\n",
      "Artists spill their colors 'neath the sky so clear,  Capturing the light that bathes Montmartre's tier.  The warmth imbues each canvas with life,  In this city where beauty and day are rife.\n",
      "Oh, Paris in sunlight stands unmatched, serene,  A vision of wonder, like a waking dream.  For even the sun seems to pause and stay,  In the city of lights, where it crafts the day.\n",
      "TERMINATE\u001b[0m\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "In Paris, the city of light and love,  \n",
      "The sun bestows warm rays from above.  \n",
      "Golden beams weave through streets so grand,  \n",
      "Caressing the stone and softening the land.\n",
      "\n",
      "In gardens where lovers often stroll,  \n",
      "Sunshine blankets each meandering knoll.  \n",
      "Reflections dance on the Seine's smooth face,  \n",
      "Gilding the waters with gentle grace.\n",
      "\n",
      "Notre Dame basks in the daylight's kiss,  \n",
      "Its spire cutting the blue, a sight of bliss.  \n",
      "In open cafes, the air tastes sweet,  \n",
      "As pastries warm in the sun's steady beat.\n",
      "\n",
      "The dappled shade of the chestnut trees,  \n",
      "Whispers of history ride on the breeze.  \n",
      "The Eiffel Tower stands tall and proud,  \n",
      "Its shadow on the city, a sun-drawn shroud.\n",
      "\n",
      "Artists spill their colors 'neath the sky so clear,  \n",
      "Capturing the light that bathes Montmartre's tier.  \n",
      "The warmth imbues each canvas with life,  \n",
      "In this city where beauty and day are rife.\n",
      "\n",
      "Oh, Paris in sunlight stands unmatched, serene,  \n",
      "A vision of wonder, like a waking dream.  \n",
      "For even the sun seems to pause and stay,  \n",
      "In the city of lights, where it crafts the day.\n",
      "\n",
      "TERMINATE\n",
      " - Received TERMINATE message. Exiting.\n"
     ]
    }
   ],
   "source": [
    "with IOWebsockets.run_server_in_thread(on_connect=on_connect, port=8765) as uri:\n",
    "    print(f\" - test_setup() with websocket server running on {uri}.\", flush=True)\n",
    "\n",
    "    with ws_connect(uri) as websocket:\n",
    "        print(f\" - Connected to server on {uri}\", flush=True)\n",
    "\n",
    "        print(\" - Sending message to server.\", flush=True)\n",
    "        # websocket.send(\"2+2=?\")\n",
    "        websocket.send(\"Check out the weather in Paris and write a poem about it.\")\n",
    "\n",
    "        while True:\n",
    "            message = websocket.recv()\n",
    "            message = message.decode(\"utf-8\") if isinstance(message, bytes) else message\n",
    "            # drop the newline character\n",
    "            if message.endswith(\"\\n\"):\n",
    "                message = message[:-1]\n",
    "\n",
    "            print(message, end=\"\", flush=True)\n",
    "\n",
    "            if \"TERMINATE\" in message:\n",
    "                print()\n",
    "                print(\" - Received TERMINATE message. Exiting.\", flush=True)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a656564",
   "metadata": {},
   "source": [
    "## Testing websockets server running inside FastAPI server with HTML/JS client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e55dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import asynccontextmanager  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "from fastapi import FastAPI  # noqa: E402\n",
    "from fastapi.responses import HTMLResponse  # noqa: E402\n",
    "\n",
    "\n",
    "PORT = 8000\n",
    "\n",
    "html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Autogen websocket test</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>WebSocket Chat</h1>\n",
    "        <form action=\"\" onsubmit=\"sendMessage(event)\">\n",
    "            <input type=\"text\" id=\"messageText\" autocomplete=\"off\"/>\n",
    "            <button>Send</button>\n",
    "        </form>\n",
    "        <ul id='messages'>\n",
    "        </ul>\n",
    "        <script>\n",
    "            var ws = new WebSocket(\"ws://localhost:8080/ws\");\n",
    "            ws.onmessage = function(event) {\n",
    "                var messages = document.getElementById('messages')\n",
    "                var message = document.createElement('li')\n",
    "                var content = document.createTextNode(event.data)\n",
    "                message.appendChild(content)\n",
    "                messages.appendChild(message)\n",
    "            };\n",
    "            function sendMessage(event) {\n",
    "                var input = document.getElementById(\"messageText\")\n",
    "                ws.send(input.value)\n",
    "                input.value = ''\n",
    "                event.preventDefault()\n",
    "            }\n",
    "        </script>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def run_websocket_server(app):\n",
    "    with IOWebsockets.run_server_in_thread(on_connect=on_connect, port=8080) as uri:\n",
    "        print(f\"Websocket server started at {uri}.\", flush=True)\n",
    "\n",
    "        yield\n",
    "\n",
    "\n",
    "app = FastAPI(lifespan=run_websocket_server)\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def get():\n",
    "    return HTMLResponse(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d92e50b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [909267]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Websocket server started at ws://127.0.0.1:8080.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [909267]\n"
     ]
    }
   ],
   "source": [
    "import uvicorn  # noqa: E402\n",
    "\n",
    "config = uvicorn.Config(app)\n",
    "server = uvicorn.Server(config)\n",
    "await server.serve()  # noqa: F704"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb50946",
   "metadata": {},
   "source": [
    "## Testing  websockets server with HTML/JS client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a98de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from http.server import HTTPServer, SimpleHTTPRequestHandler  # noqa: E402\n",
    "\n",
    "PORT = 8000\n",
    "\n",
    "html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Autogen websocket test</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>WebSocket Chat</h1>\n",
    "        <form action=\"\" onsubmit=\"sendMessage(event)\">\n",
    "            <input type=\"text\" id=\"messageText\" autocomplete=\"off\"/>\n",
    "            <button>Send</button>\n",
    "        </form>\n",
    "        <ul id='messages'>\n",
    "        </ul>\n",
    "        <script>\n",
    "            var ws = new WebSocket(\"ws://localhost:8080/ws\");\n",
    "            ws.onmessage = function(event) {\n",
    "                var messages = document.getElementById('messages')\n",
    "                var message = document.createElement('li')\n",
    "                var content = document.createTextNode(event.data)\n",
    "                message.appendChild(content)\n",
    "                messages.appendChild(message)\n",
    "            };\n",
    "            function sendMessage(event) {\n",
    "                var input = document.getElementById(\"messageText\")\n",
    "                ws.send(input.value)\n",
    "                input.value = ''\n",
    "                event.preventDefault()\n",
    "            }\n",
    "        </script>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "    # create a simple HTTP webpage\n",
    "    path = Path(temp_dir) / \"chat.html\"\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(html)\n",
    "\n",
    "    #\n",
    "    class MyRequestHandler(SimpleHTTPRequestHandler):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, directory=temp_dir, **kwargs)\n",
    "\n",
    "        def do_GET(self):\n",
    "            if self.path == \"/\":\n",
    "                self.path = \"/chat.html\"\n",
    "            return SimpleHTTPRequestHandler.do_GET(self)\n",
    "\n",
    "    handler = MyRequestHandler\n",
    "\n",
    "    with IOWebsockets.run_server_in_thread(on_connect=on_connect, port=8080) as uri:\n",
    "        print(f\"Websocket server started at {uri}.\", flush=True)\n",
    "\n",
    "        with HTTPServer((\"\", PORT), handler) as httpd:\n",
    "            print(\"HTTP server started at http://localhost:\" + str(PORT))\n",
    "            httpd.serve_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19656d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
