{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Auto Generated Agent Chat: Society of Minds\n",
    "\n",
    "This notebook showcases how to use conversable agents in AutoGen to realize a Society of Mind (SoM) scenario, in which an agent runs a  chat session with another agent as an inner-monologue. The idea of SoM and part of the example demonstrated in this notebook come from this PR: https://github.com/microsoft/autogen/pull/890\n",
    " \n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from typing import Union\n",
    "\n",
    "# test Gurobi installation\n",
    "from gurobipy import GRB\n",
    "from eventlet.timeout import Timeout\n",
    "import re\n",
    "from termcolor import colored\n",
    "from autogen.code_utils import extract_code\n",
    "\n",
    "# import auxillary packages\n",
    "import requests  # for loading the example source code\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    # filter_dict={\n",
    "    #     \"model\": [\"gpt-4\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    # },\n",
    ")\n",
    "llm_config = {\"config_list\": config_list_gpt4}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OptiGuide with Nested Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import time\n",
      "\n",
      "from gurobipy import GRB, Model\n",
      "\n",
      "# Example data\n",
      "\n",
      "capacity_in_supplier = {'supplier1': 150, 'supplier2': 50, 'supplier3': 100}\n",
      "\n",
      "shipping_cost_from_supplier_to_roastery = {\n",
      "    ('supplier1', 'roastery1'): 5,\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "# Solve\n",
      "m.update()\n",
      "model.optimize()\n",
      "\n",
      "print(time.ctime())\n",
      "if m.status == GRB.OPTIMAL:\n",
      "    print(f'Optimal cost: {m.objVal}')\n",
      "else:\n",
      "    print(\"Not solved to optimality. Optimization status:\", m.status)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% System Messages\n",
    "WRITER_SYSTEM_MSG = \"\"\"You are a chatbot to:\n",
    "(1) write Python code to answer users questions for supply chain-related coding\n",
    "project;\n",
    "(2) explain solutions from a Gurobi/Python solver.\n",
    "\n",
    "--- SOURCE CODE ---\n",
    "{source_code}\n",
    "\n",
    "--- DOC STR ---\n",
    "{doc_str}\n",
    "---\n",
    "\n",
    "Here are some example questions and their answers and codes:\n",
    "--- EXAMPLES ---\n",
    "{example_qa}\n",
    "---\n",
    "\n",
    "The execution result of the original source code is below.\n",
    "--- Original Result ---\n",
    "{execution_result}\n",
    "\n",
    "Note that your written code will be added to the lines with substring:\n",
    "\"# OPTIGUIDE *** CODE GOES HERE\"\n",
    "So, you don't need to write other code, such as m.optimize() or m.update().\n",
    "You just need to write code snippet in ```python ...``` block.\n",
    "\"\"\"\n",
    "\n",
    "SAFEGUARD_SYSTEM_MSG = \"\"\"\n",
    "Given the original source code:\n",
    "{source_code}\n",
    "\n",
    "Is the following code safe (not malicious code to break security\n",
    "and privacy) to run?\n",
    "Answer only one word.\n",
    "If not safe, answer `DANGER`; else, answer `SAFE`.\n",
    "\"\"\"\n",
    "\n",
    "# %% Constant strings to match code lines in the source code.\n",
    "DATA_CODE_STR = \"# OPTIGUIDE DATA CODE GOES HERE\"\n",
    "CONSTRAINT_CODE_STR = \"# OPTIGUIDE CONSTRAINT CODE GOES HERE\"\n",
    "\n",
    "# In-context learning examples.\n",
    "example_qa = \"\"\"\n",
    "----------\n",
    "Question: Why is it not recommended to use just one supplier for roastery 2?\n",
    "Answer Code:\n",
    "```python\n",
    "z = m.addVars(suppliers, vtype=GRB.BINARY, name=\"z\")\n",
    "m.addConstr(sum(z[s] for s in suppliers) <= 1, \"_\")\n",
    "for s in suppliers:\n",
    "    m.addConstr(x[s,'roastery2'] <= capacity_in_supplier[s] * z[s], \"_\")\n",
    "```\n",
    "\n",
    "----------\n",
    "Question: What if there's a 13% jump in the demand for light coffee at cafe1?\n",
    "Answer Code:\n",
    "```python\n",
    "light_coffee_needed_for_cafe[\"cafe1\"] = light_coffee_needed_for_cafe[\"cafe1\"] * (1 + 13/100)\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "CODE_PROMPT = \"\"\"\n",
    "Answer Code:\n",
    "\"\"\"\n",
    "\n",
    "DEBUG_PROMPT = \"\"\"\n",
    "\n",
    "While running the code you suggested, I encountered the {error_type}:\n",
    "--- ERROR MESSAGE ---\n",
    "{error_message}\n",
    "\n",
    "Please try to resolve this bug, and rewrite the code snippet.\n",
    "--- NEW CODE ---\n",
    "\"\"\"\n",
    "\n",
    "SAFEGUARD_PROMPT = \"\"\"\n",
    "--- Code ---\n",
    "{code}\n",
    "\n",
    "--- One-Word Answer: SAFE or DANGER ---\n",
    "\"\"\"\n",
    "\n",
    "INTERPRETER_PROMPT = \"\"\"Here are the execution results: {execution_rst}\n",
    "\n",
    "Can you organize these information to a human readable answer?\n",
    "Remember to compare the new results to the original results you obtained in the\n",
    "beginning.\n",
    "\n",
    "--- HUMAN READABLE ANSWER ---\n",
    "\"\"\"\n",
    "\n",
    "# Get the source code of the coffee example from OptiGuide's official repo\n",
    "code_url = \"https://raw.githubusercontent.com/microsoft/OptiGuide/main/benchmark/application/coffee.py\"\n",
    "response = requests.get(code_url)\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Get the text content from the response\n",
    "    code = response.text\n",
    "else:\n",
    "    raise RuntimeError(\"Failed to retrieve the file.\")\n",
    "# code = open(code_url, \"r\").read() # for local files\n",
    "\n",
    "\n",
    "# show the first head and tail of the source code\n",
    "print(\"\\n\".join(code.split(\"\\n\")[:10]))\n",
    "print(\".\\n\" * 3)\n",
    "print(\"\\n\".join(code.split(\"\\n\")[-10:]))\n",
    "\n",
    "writer_system_msg = WRITER_SYSTEM_MSG.format(\n",
    "    source_code=code,\n",
    "    doc_str=\"\",\n",
    "    example_qa=example_qa,\n",
    "    execution_result=\"\",\n",
    ")\n",
    "safeguard_system_msg = SAFEGUARD_SYSTEM_MSG.format(source_code=code)\n",
    "# TODO: system message needs to be changed. Needs to add execution result of the original source code and user_chat_history\n",
    "\n",
    "\n",
    "def replace(src_code: str, old_code: str, new_code: str) -> str:\n",
    "    \"\"\"\n",
    "    Inserts new code into the source code by replacing a specified old\n",
    "    code block.\n",
    "\n",
    "    Args:\n",
    "        src_code (str): The source code to modify.\n",
    "        old_code (str): The code block to be replaced.\n",
    "        new_code (str): The new code block to insert.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified source code with the new code inserted.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        src_code = 'def hello_world():\\n    print(\"Hello, world!\")\\n\\n# Some\n",
    "        other code here'\n",
    "        old_code = 'print(\"Hello, world!\")'\n",
    "        new_code = 'print(\"Bonjour, monde!\")\\nprint(\"Hola, mundo!\")'\n",
    "        modified_code = _replace(src_code, old_code, new_code)\n",
    "        print(modified_code)\n",
    "        # Output:\n",
    "        # def hello_world():\n",
    "        #     print(\"Bonjour, monde!\")\n",
    "        #     print(\"Hola, mundo!\")\n",
    "        # Some other code here\n",
    "    \"\"\"\n",
    "    pattern = r\"( *){old_code}\".format(old_code=old_code)\n",
    "    head_spaces = re.search(pattern, src_code, flags=re.DOTALL).group(1)\n",
    "    new_code = \"\\n\".join([head_spaces + line for line in new_code.split(\"\\n\")])\n",
    "    rst = re.sub(pattern, new_code, src_code)\n",
    "    return rst\n",
    "\n",
    "\n",
    "def insert_code(src_code: str, new_lines: str) -> str:\n",
    "    \"\"\"insert a code patch into the source code.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        src_code (str): the full source code\n",
    "        new_lines (str): The new code.\n",
    "\n",
    "    Returns:\n",
    "        str: the full source code after insertion (replacement).\n",
    "    \"\"\"\n",
    "    if new_lines.find(\"addConstr\") >= 0:\n",
    "        return replace(src_code, CONSTRAINT_CODE_STR, new_lines)\n",
    "    else:\n",
    "        return replace(src_code, DATA_CODE_STR, new_lines)\n",
    "\n",
    "\n",
    "def run_with_exec(src_code: str) -> Union[str, Exception]:\n",
    "    \"\"\"Run the code snippet with exec.\n",
    "\n",
    "    Args:\n",
    "        src_code (str): The source code to run.\n",
    "\n",
    "    Returns:\n",
    "        object: The result of the code snippet.\n",
    "            If the code succeed, returns the objective value (float or string).\n",
    "            else, return the error (exception)\n",
    "    \"\"\"\n",
    "    locals_dict = {}\n",
    "    locals_dict.update(globals())\n",
    "    locals_dict.update(locals())\n",
    "\n",
    "    timeout = Timeout(\n",
    "        60,\n",
    "        TimeoutError(\"This is a timeout exception, in case \" \"GPT's code falls into infinite loop.\"),\n",
    "    )\n",
    "    try:\n",
    "        exec(src_code, locals_dict, locals_dict)\n",
    "    except Exception as e:\n",
    "        return e\n",
    "    finally:\n",
    "        timeout.cancel()\n",
    "\n",
    "    try:\n",
    "        status = locals_dict[\"m\"].Status\n",
    "        if status != GRB.OPTIMAL:\n",
    "            if status == GRB.UNBOUNDED:\n",
    "                ans = \"unbounded\"\n",
    "            elif status == GRB.INF_OR_UNBD:\n",
    "                ans = \"inf_or_unbound\"\n",
    "            elif status == GRB.INFEASIBLE:\n",
    "                ans = \"infeasible\"\n",
    "                m = locals_dict[\"m\"]\n",
    "                m.computeIIS()\n",
    "                constrs = [c.ConstrName for c in m.getConstrs() if c.IISConstr]\n",
    "                ans += \"\\nConflicting Constraints:\\n\" + str(constrs)\n",
    "            else:\n",
    "                ans = \"Model Status:\" + str(status)\n",
    "        else:\n",
    "            ans = \"Optimization problem solved. The objective value is: \" + str(locals_dict[\"m\"].objVal)\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptiGuide(autogen.AssistantAgent):\n",
    "    source_code: str = code\n",
    "    debug_times: int = 3\n",
    "    debug_times_left: int = 3\n",
    "    example_qa: str = \"\"\n",
    "    success: bool = False\n",
    "    user_chat_history: str = \"\"\n",
    "\n",
    "    def set_success(self, success: bool):\n",
    "        self.success = success\n",
    "\n",
    "    @property\n",
    "    def get_success(self):\n",
    "        return self.success\n",
    "\n",
    "    def update_debug_times(self):\n",
    "        self.debug_times_left -= 1\n",
    "\n",
    "    def set_user_chat_history(self, user_chat_history: str):\n",
    "        self.user_chat_history = user_chat_history\n",
    "\n",
    "\n",
    "class Writer(autogen.AssistantAgent):\n",
    "    source_code: str = code\n",
    "    debug_times: int = 3\n",
    "    debug_times_left: int = 3\n",
    "    example_qa: str = \"\"\n",
    "    user_chat_history: str = \"\"\n",
    "\n",
    "    def set_user_chat_history(self, user_chat_history: str):\n",
    "        self.user_chat_history = user_chat_history\n",
    "\n",
    "\n",
    "writer = Writer(\"writer\", llm_config=llm_config)\n",
    "safeguard = autogen.AssistantAgent(\"safeguard\", llm_config=llm_config)\n",
    "optiguide_commander = OptiGuide(\"commander\", llm_config=llm_config)\n",
    "\n",
    "user = autogen.UserProxyAgent(\n",
    "    \"user\", max_consecutive_auto_reply=0, human_input_mode=\"NEVER\", code_execution_config=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register reply func and nested chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_writer_reply(chat_queue, recipient, messages, sender, config):\n",
    "    # max_turn = 10\n",
    "    # if chat_queue[0][\"recipient\"].reply_num >= max_turn:\n",
    "    #     return True, None\n",
    "    msg_content = messages[-1].get(\"content\", \"\")\n",
    "    # board = config\n",
    "    # get execution result of the original source code\n",
    "    sender_history = recipient.chat_messages[sender]\n",
    "    user_chat_history = \"\\nHere are the history of discussions:\\n\" f\"{sender_history}\"\n",
    "\n",
    "    if sender.name == \"user\":\n",
    "        execution_result = msg_content  # TODO: get the execution result of the original source code\n",
    "    else:\n",
    "        execution_result = \"\"\n",
    "    writer_sys_msg = (\n",
    "        WRITER_SYSTEM_MSG.format(\n",
    "            source_code=recipient.source_code,\n",
    "            doc_str=\"\",\n",
    "            example_qa=example_qa,\n",
    "            execution_result=execution_result,\n",
    "        )\n",
    "        + user_chat_history\n",
    "    )\n",
    "    # safeguard_sys_msg = (\n",
    "    #             SAFEGUARD_SYSTEM_MSG.format(source_code=recipient.source_code)\n",
    "    #             + user_chat_history\n",
    "    #         )\n",
    "    # update system message\n",
    "    nested_agent = chat_queue[0][\"recipient\"]\n",
    "    nested_agent.update_system_message(writer_sys_msg)\n",
    "    print(\"nestss\", nested_agent.name)\n",
    "    nested_agent.set_user_chat_history(user_chat_history)\n",
    "    # safeguard.update_system_message(safeguard_sys_msg) #TODO: update system message for safeguard\n",
    "    nested_agent.reset()\n",
    "    # safeguard.reset() #TODO: reset safeguard\n",
    "    recipient.debug_times_left = recipient.debug_times\n",
    "    recipient.set_success(False)\n",
    "\n",
    "    chat_queue[0][\"recipient\"] = nested_agent\n",
    "    chat_queue[0][\"message\"] = CODE_PROMPT\n",
    "\n",
    "    chat_res = recipient.initiate_chats(chat_queue)\n",
    "\n",
    "    if recipient.success:\n",
    "        reply = list(chat_res.values())[-1].summary\n",
    "    else:\n",
    "        reply = \"Sorry. I cannot answer your question.\"\n",
    "    return True, reply\n",
    "\n",
    "\n",
    "def nested_safeguard_reply(chat_queue, recipient, messages, sender, config):\n",
    "    if recipient.success:\n",
    "        # no reply to writer\n",
    "        return True, None\n",
    "    # Step 3: safeguard\n",
    "\n",
    "    safeguard_sys_msg = SAFEGUARD_SYSTEM_MSG.format(source_code=recipient.source_code) + recipient.user_chat_history\n",
    "\n",
    "    chat_agent = chat_queue[0][\"recipient\"]\n",
    "    chat_agent.update_system_message(safeguard_sys_msg)\n",
    "    chat_agent.reset()\n",
    "    chat_queue[0][\"recipient\"] = chat_agent\n",
    "    # chat_agent[0][\"recipient\"].set_success(False)\n",
    "    last_msg_content = messages[-1].get(\"content\", \"\")\n",
    "    _, code = extract_code(last_msg_content)[0]\n",
    "    chat_info = chat_queue[0]\n",
    "    chat_info[\"message\"] = SAFEGUARD_SYSTEM_MSG.format(source_code=recipient.source_code)\n",
    "    chat_queue[0] = chat_info\n",
    "    res = recipient.initiate_chats(chat_queue)\n",
    "\n",
    "    safe_msg = list(res.values())[-1].summary\n",
    "    if safe_msg.find(\"DANGER\") < 0:\n",
    "        # Step 4 and 5: Run the code and obtain the results\n",
    "        src_code = insert_code(recipient.source_code, code)\n",
    "        execution_rst = run_with_exec(src_code)\n",
    "        print(colored(str(execution_rst), \"yellow\"))\n",
    "        if type(execution_rst) in [str, int, float]:\n",
    "            # we successfully run the code and get the result\n",
    "            recipient.set_success(True)\n",
    "            # Step 6: request to interpret results\n",
    "            return True, INTERPRETER_PROMPT.format(execution_rst=execution_rst)\n",
    "    else:\n",
    "        # DANGER: If not safe, try to debug. Redo coding\n",
    "        execution_rst = \"\"\"\n",
    "        Sorry, this new code is not safe to run. I would not allow you to execute it.\n",
    "        Please try to find a new way (coding) to answer the question.\"\"\"\n",
    "        if recipient.debug_times_left > 0:\n",
    "            # Try to debug and write code again (back to step 2)\n",
    "            recipient.update_debug_times()\n",
    "            return True, DEBUG_PROMPT.format(error_type=type(execution_rst), error_message=str(execution_rst))\n",
    "\n",
    "\n",
    "writer_chat_queue = [{\"recipient\": writer, \"summary_method\": \"reflection_with_llm\"}]\n",
    "safeguard_chat_queue = [{\"recipient\": safeguard, \"summary_method\": \"last_msg\"}]\n",
    "# writer is triggered only when receiving a message from the user\n",
    "optiguide_commander.register_nested_chats(\"user\", writer_chat_queue, nested_writer_reply)\n",
    "# safeguard is triggered only when receiving a message from the writer\n",
    "optiguide_commander.register_nested_chats(\"writer\", safeguard_chat_queue, nested_safeguard_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let agents talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to commander):\n",
      "\n",
      "What if we prohibit shipping from supplier 1 to roastery 2?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "nestss writer\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStart a new chat with the following message: \n",
      "\n",
      "Answer Code:\n",
      "\n",
      "\n",
      "With the following carryover: \n",
      "\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mcommander\u001b[0m (to writer):\n",
      "\n",
      "\n",
      "Answer Code:\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mwriter\u001b[0m (to commander):\n",
      "\n",
      "```python\n",
      "model.addConstr(x['supplier1', 'roastery2'] == 0, \"prohibit_s1_r2\")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStart a new chat with the following message: \n",
      "\n",
      "Given the original source code:\n",
      "import time\n",
      "\n",
      "from gurobipy import GRB, Model\n",
      "\n",
      "# Example data\n",
      "\n",
      "capacity_in_supplier = {'supplier1': 150, 'supplier2': 50, 'supplier3': 100}\n",
      "\n",
      "shipping_cost_from_supplier_to_roastery = {\n",
      "    ('supplier1', 'roastery1'): 5,\n",
      "    ('supplier1', 'roastery2'): 4,\n",
      "    ('supplier2', 'roastery1'): 6,\n",
      "    ('supplier2', 'roastery2'): 3,\n",
      "    ('supplier3', 'roastery1'): 2,\n",
      "    ('supplier3', 'roastery2'): 7\n",
      "}\n",
      "\n",
      "roasting_cost_light = {'roastery1': 3, 'roastery2': 5}\n",
      "\n",
      "roasting_cost_dark = {'roastery1': 5, 'roastery2': 6}\n",
      "\n",
      "shipping_cost_from_roastery_to_cafe = {\n",
      "    ('roastery1', 'cafe1'): 5,\n",
      "    ('roastery1', 'cafe2'): 3,\n",
      "    ('roastery1', 'cafe3'): 6,\n",
      "    ('roastery2', 'cafe1'): 4,\n",
      "    ('roastery2', 'cafe2'): 5,\n",
      "    ('roastery2', 'cafe3'): 2\n",
      "}\n",
      "\n",
      "light_coffee_needed_for_cafe = {'cafe1': 20, 'cafe2': 30, 'cafe3': 40}\n",
      "\n",
      "dark_coffee_needed_for_cafe = {'cafe1': 20, 'cafe2': 20, 'cafe3': 100}\n",
      "\n",
      "cafes = list(set(i[1] for i in shipping_cost_from_roastery_to_cafe.keys()))\n",
      "roasteries = list(\n",
      "    set(i[1] for i in shipping_cost_from_supplier_to_roastery.keys()))\n",
      "suppliers = list(\n",
      "    set(i[0] for i in shipping_cost_from_supplier_to_roastery.keys()))\n",
      "\n",
      "# Create a new model\n",
      "model = Model(\"coffee_distribution\")\n",
      "\n",
      "# OPTIGUIDE DATA CODE GOES HERE\n",
      "\n",
      "# Create variables\n",
      "x = model.addVars(shipping_cost_from_supplier_to_roastery.keys(),\n",
      "                  vtype=GRB.INTEGER,\n",
      "                  name=\"x\")\n",
      "y_light = model.addVars(shipping_cost_from_roastery_to_cafe.keys(),\n",
      "                        vtype=GRB.INTEGER,\n",
      "                        name=\"y_light\")\n",
      "y_dark = model.addVars(shipping_cost_from_roastery_to_cafe.keys(),\n",
      "                       vtype=GRB.INTEGER,\n",
      "                       name=\"y_dark\")\n",
      "\n",
      "# Set objective\n",
      "model.setObjective(\n",
      "    sum(x[i] * shipping_cost_from_supplier_to_roastery[i]\n",
      "        for i in shipping_cost_from_supplier_to_roastery.keys()) +\n",
      "    sum(roasting_cost_light[r] * y_light[r, c] +\n",
      "        roasting_cost_dark[r] * y_dark[r, c]\n",
      "        for r, c in shipping_cost_from_roastery_to_cafe.keys()) + sum(\n",
      "            (y_light[j] + y_dark[j]) * shipping_cost_from_roastery_to_cafe[j]\n",
      "            for j in shipping_cost_from_roastery_to_cafe.keys()), GRB.MINIMIZE)\n",
      "\n",
      "# Conservation of flow constraint\n",
      "for r in set(i[1] for i in shipping_cost_from_supplier_to_roastery.keys()):\n",
      "    model.addConstr(\n",
      "        sum(x[i] for i in shipping_cost_from_supplier_to_roastery.keys()\n",
      "            if i[1] == r) == sum(\n",
      "                y_light[j] + y_dark[j]\n",
      "                for j in shipping_cost_from_roastery_to_cafe.keys()\n",
      "                if j[0] == r), f\"flow_{r}\")\n",
      "\n",
      "# Add supply constraints\n",
      "for s in set(i[0] for i in shipping_cost_from_supplier_to_roastery.keys()):\n",
      "    model.addConstr(\n",
      "        sum(x[i] for i in shipping_cost_from_supplier_to_roastery.keys()\n",
      "            if i[0] == s) <= capacity_in_supplier[s], f\"supply_{s}\")\n",
      "\n",
      "# Add demand constraints\n",
      "for c in set(i[1] for i in shipping_cost_from_roastery_to_cafe.keys()):\n",
      "    model.addConstr(\n",
      "        sum(y_light[j] for j in shipping_cost_from_roastery_to_cafe.keys()\n",
      "            if j[1] == c) >= light_coffee_needed_for_cafe[c],\n",
      "        f\"light_demand_{c}\")\n",
      "    model.addConstr(\n",
      "        sum(y_dark[j] for j in shipping_cost_from_roastery_to_cafe.keys()\n",
      "            if j[1] == c) >= dark_coffee_needed_for_cafe[c],\n",
      "        f\"dark_demand_{c}\")\n",
      "\n",
      "# Optimize model\n",
      "model.optimize()\n",
      "m = model\n",
      "\n",
      "# OPTIGUIDE CONSTRAINT CODE GOES HERE\n",
      "\n",
      "# Solve\n",
      "m.update()\n",
      "model.optimize()\n",
      "\n",
      "print(time.ctime())\n",
      "if m.status == GRB.OPTIMAL:\n",
      "    print(f'Optimal cost: {m.objVal}')\n",
      "else:\n",
      "    print(\"Not solved to optimality. Optimization status:\", m.status)\n",
      "\n",
      "\n",
      "Is the following code safe (not malicious code to break security\n",
      "and privacy) to run?\n",
      "Answer only one word.\n",
      "If not safe, answer `DANGER`; else, answer `SAFE`.\n",
      "\n",
      "\n",
      "With the following carryover: \n",
      "\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mcommander\u001b[0m (to safeguard):\n",
      "\n",
      "\n",
      "Given the original source code:\n",
      "import time\n",
      "\n",
      "from gurobipy import GRB, Model\n",
      "\n",
      "# Example data\n",
      "\n",
      "capacity_in_supplier = {'supplier1': 150, 'supplier2': 50, 'supplier3': 100}\n",
      "\n",
      "shipping_cost_from_supplier_to_roastery = {\n",
      "    ('supplier1', 'roastery1'): 5,\n",
      "    ('supplier1', 'roastery2'): 4,\n",
      "    ('supplier2', 'roastery1'): 6,\n",
      "    ('supplier2', 'roastery2'): 3,\n",
      "    ('supplier3', 'roastery1'): 2,\n",
      "    ('supplier3', 'roastery2'): 7\n",
      "}\n",
      "\n",
      "roasting_cost_light = {'roastery1': 3, 'roastery2': 5}\n",
      "\n",
      "roasting_cost_dark = {'roastery1': 5, 'roastery2': 6}\n",
      "\n",
      "shipping_cost_from_roastery_to_cafe = {\n",
      "    ('roastery1', 'cafe1'): 5,\n",
      "    ('roastery1', 'cafe2'): 3,\n",
      "    ('roastery1', 'cafe3'): 6,\n",
      "    ('roastery2', 'cafe1'): 4,\n",
      "    ('roastery2', 'cafe2'): 5,\n",
      "    ('roastery2', 'cafe3'): 2\n",
      "}\n",
      "\n",
      "light_coffee_needed_for_cafe = {'cafe1': 20, 'cafe2': 30, 'cafe3': 40}\n",
      "\n",
      "dark_coffee_needed_for_cafe = {'cafe1': 20, 'cafe2': 20, 'cafe3': 100}\n",
      "\n",
      "cafes = list(set(i[1] for i in shipping_cost_from_roastery_to_cafe.keys()))\n",
      "roasteries = list(\n",
      "    set(i[1] for i in shipping_cost_from_supplier_to_roastery.keys()))\n",
      "suppliers = list(\n",
      "    set(i[0] for i in shipping_cost_from_supplier_to_roastery.keys()))\n",
      "\n",
      "# Create a new model\n",
      "model = Model(\"coffee_distribution\")\n",
      "\n",
      "# OPTIGUIDE DATA CODE GOES HERE\n",
      "\n",
      "# Create variables\n",
      "x = model.addVars(shipping_cost_from_supplier_to_roastery.keys(),\n",
      "                  vtype=GRB.INTEGER,\n",
      "                  name=\"x\")\n",
      "y_light = model.addVars(shipping_cost_from_roastery_to_cafe.keys(),\n",
      "                        vtype=GRB.INTEGER,\n",
      "                        name=\"y_light\")\n",
      "y_dark = model.addVars(shipping_cost_from_roastery_to_cafe.keys(),\n",
      "                       vtype=GRB.INTEGER,\n",
      "                       name=\"y_dark\")\n",
      "\n",
      "# Set objective\n",
      "model.setObjective(\n",
      "    sum(x[i] * shipping_cost_from_supplier_to_roastery[i]\n",
      "        for i in shipping_cost_from_supplier_to_roastery.keys()) +\n",
      "    sum(roasting_cost_light[r] * y_light[r, c] +\n",
      "        roasting_cost_dark[r] * y_dark[r, c]\n",
      "        for r, c in shipping_cost_from_roastery_to_cafe.keys()) + sum(\n",
      "            (y_light[j] + y_dark[j]) * shipping_cost_from_roastery_to_cafe[j]\n",
      "            for j in shipping_cost_from_roastery_to_cafe.keys()), GRB.MINIMIZE)\n",
      "\n",
      "# Conservation of flow constraint\n",
      "for r in set(i[1] for i in shipping_cost_from_supplier_to_roastery.keys()):\n",
      "    model.addConstr(\n",
      "        sum(x[i] for i in shipping_cost_from_supplier_to_roastery.keys()\n",
      "            if i[1] == r) == sum(\n",
      "                y_light[j] + y_dark[j]\n",
      "                for j in shipping_cost_from_roastery_to_cafe.keys()\n",
      "                if j[0] == r), f\"flow_{r}\")\n",
      "\n",
      "# Add supply constraints\n",
      "for s in set(i[0] for i in shipping_cost_from_supplier_to_roastery.keys()):\n",
      "    model.addConstr(\n",
      "        sum(x[i] for i in shipping_cost_from_supplier_to_roastery.keys()\n",
      "            if i[0] == s) <= capacity_in_supplier[s], f\"supply_{s}\")\n",
      "\n",
      "# Add demand constraints\n",
      "for c in set(i[1] for i in shipping_cost_from_roastery_to_cafe.keys()):\n",
      "    model.addConstr(\n",
      "        sum(y_light[j] for j in shipping_cost_from_roastery_to_cafe.keys()\n",
      "            if j[1] == c) >= light_coffee_needed_for_cafe[c],\n",
      "        f\"light_demand_{c}\")\n",
      "    model.addConstr(\n",
      "        sum(y_dark[j] for j in shipping_cost_from_roastery_to_cafe.keys()\n",
      "            if j[1] == c) >= dark_coffee_needed_for_cafe[c],\n",
      "        f\"dark_demand_{c}\")\n",
      "\n",
      "# Optimize model\n",
      "model.optimize()\n",
      "m = model\n",
      "\n",
      "# OPTIGUIDE CONSTRAINT CODE GOES HERE\n",
      "\n",
      "# Solve\n",
      "m.update()\n",
      "model.optimize()\n",
      "\n",
      "print(time.ctime())\n",
      "if m.status == GRB.OPTIMAL:\n",
      "    print(f'Optimal cost: {m.objVal}')\n",
      "else:\n",
      "    print(\"Not solved to optimality. Optimization status:\", m.status)\n",
      "\n",
      "\n",
      "Is the following code safe (not malicious code to break security\n",
      "and privacy) to run?\n",
      "Answer only one word.\n",
      "If not safe, answer `DANGER`; else, answer `SAFE`.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msafeguard\u001b[0m (to commander):\n",
      "\n",
      "SAFE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcommander\u001b[0m (to safeguard):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Gurobi Optimizer version 11.0.0 build v11.0.0rc2 (mac64[arm] - Darwin 23.2.0 23C71)\n",
      "\n",
      "CPU model: Apple M3 Max\n",
      "Thread count: 14 physical cores, 14 logical processors, using up to 14 threads\n",
      "\n",
      "Optimize a model with 11 rows, 18 columns and 36 nonzeros\n",
      "Model fingerprint: 0xab8d681d\n",
      "Variable types: 0 continuous, 18 integer (0 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [2e+00, 1e+01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+01, 2e+02]\n",
      "Found heuristic solution: objective 2900.0000000\n",
      "Presolve time: 0.00s\n",
      "Presolved: 11 rows, 18 columns, 36 nonzeros\n",
      "Variable types: 0 continuous, 18 integer (0 binary)\n",
      "Found heuristic solution: objective 2896.0000000\n",
      "\n",
      "Root relaxation: objective 2.470000e+03, 11 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0    2470.0000000 2470.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (11 simplex iterations) in 0.00 seconds (0.00 work units)\n",
      "Thread count was 14 (of 14 available processors)\n",
      "\n",
      "Solution count 3: 2470 2896 2900 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.470000000000e+03, best bound 2.470000000000e+03, gap 0.0000%\n",
      "Gurobi Optimizer version 11.0.0 build v11.0.0rc2 (mac64[arm] - Darwin 23.2.0 23C71)\n",
      "\n",
      "CPU model: Apple M3 Max\n",
      "Thread count: 14 physical cores, 14 logical processors, using up to 14 threads\n",
      "\n",
      "Optimize a model with 12 rows, 18 columns and 37 nonzeros\n",
      "Model fingerprint: 0x5dab87a5\n",
      "Variable types: 0 continuous, 18 integer (0 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [2e+00, 1e+01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+01, 2e+02]\n",
      "\n",
      "MIP start from previous solve did not produce a new incumbent solution\n",
      "MIP start from previous solve violates constraint prohibit_s1_r2 by 80.000000000\n",
      "\n",
      "Found heuristic solution: objective 2920.0000000\n",
      "Presolve removed 2 rows and 1 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 10 rows, 17 columns, 33 nonzeros\n",
      "Variable types: 0 continuous, 17 integer (0 binary)\n",
      "\n",
      "Root relaxation: objective 2.760000e+03, 10 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0    2760.0000000 2760.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (10 simplex iterations) in 0.00 seconds (0.00 work units)\n",
      "Thread count was 14 (of 14 available processors)\n",
      "\n",
      "Solution count 2: 2760 2920 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.760000000000e+03, best bound 2.760000000000e+03, gap 0.0000%\n",
      "Thu Feb  8 22:01:13 2024\n",
      "Optimal cost: 2760.0\n",
      "\u001b[33mOptimization problem solved. The objective value is: 2760.0\u001b[0m\n",
      "\u001b[33mcommander\u001b[0m (to writer):\n",
      "\n",
      "Here are the execution results: Optimization problem solved. The objective value is: 2760.0\n",
      "\n",
      "Can you organize these information to a human readable answer?\n",
      "Remember to compare the new results to the original results you obtained in the\n",
      "beginning.\n",
      "\n",
      "--- HUMAN READABLE ANSWER ---\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mwriter\u001b[0m (to commander):\n",
      "\n",
      "Certainly, the results from optimizing the supply chain model suggest the following:\n",
      "\n",
      "After adding the constraint to prohibit shipping from supplier 1 to roastery 2, the optimization problem was still successfully solved. The new objective value, which represents the total cost of the supply chain operation after implementing the constraint, is $2760.0.\n",
      "\n",
      "To provide a comparison with the original results, we should look at the objective value before the constraint was added. If the previous objective value is lower than $2760.0, this implies that the new constraint increased the total cost of operations, highlighting the economic impact of not using supplier 1 for roastery 2.\n",
      "\n",
      "On the other hand, if the previous objective value is higher than $2760.0, then despite the constraint, we have found a more cost-effective way to meet the supply and demand needs.\n",
      "\n",
      "Unfortunately, the original objective value prior to this constraint being added is not provided here. For a full comparison, you would need that original value to assess the financial impact of this constraint on the overall supply chain cost.\n",
      "\n",
      "--- HUMAN READABLE ANSWER ---\n",
      "The new optimization, with shipping from supplier 1 to roastery 2 prohibited, resulted in a total cost of $2760.0. Without the initial result provided, it is difficult to directly compare, but this new result sets a benchmark for cost with the current constraints in place. To fully understand the impact, the objective value without this constraint (from the original model) must be reviewed for comparison.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcommander\u001b[0m (to user):\n",
      "\n",
      "The optimization of the supply chain model with a constraint that prohibits shipping from supplier 1 to roastery 2 was successful, and the calculated total operation cost is $2760.0. To determine the impact of this constraint, a comparison with the original objective value is needed; however, it was not provided in the conversation. If the original value is lower, the constraint has increased costs; if it is higher, the supply chain might be more cost-effective despite the constraint.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_history=[{'content': 'What if we prohibit shipping from supplier 1 to roastery 2?', 'role': 'assistant'}, {'content': 'The optimization of the supply chain model with a constraint that prohibits shipping from supplier 1 to roastery 2 was successful, and the calculated total operation cost is $2760.0. To determine the impact of this constraint, a comparison with the original objective value is needed; however, it was not provided in the conversation. If the original value is lower, the constraint has increased costs; if it is higher, the supply chain might be more cost-effective despite the constraint.', 'role': 'user'}], summary='The optimization of the supply chain model with a constraint that prohibits shipping from supplier 1 to roastery 2 was successful, and the calculated total operation cost is $2760.0. To determine the impact of this constraint, a comparison with the original objective value is needed; however, it was not provided in the conversation. If the original value is lower, the constraint has increased costs; if it is higher, the supply chain might be more cost-effective despite the constraint.', cost=({'total_cost': 0.0465, 'gpt-4': {'cost': 0.0465, 'prompt_tokens': 1544, 'completion_tokens': 3, 'total_tokens': 1547}}, {'total_cost': 0}), human_input=[])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.initiate_chat(optiguide_commander, message=\"What if we prohibit shipping from supplier 1 to roastery 2?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2d910cfd2d2a4fc49fc30fbbdc5576a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "454146d0f7224f038689031002906e6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e4ae2b6f5a974fd4bafb6abb9d12ff26",
        "IPY_MODEL_577e1e3cc4db4942b0883577b3b52755",
        "IPY_MODEL_b40bdfb1ac1d4cffb7cefcb870c64d45"
       ],
       "layout": "IPY_MODEL_dc83c7bff2f241309537a8119dfc7555",
       "tabbable": null,
       "tooltip": null
      }
     },
     "577e1e3cc4db4942b0883577b3b52755": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2d910cfd2d2a4fc49fc30fbbdc5576a7",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_74a6ba0c3cbc4051be0a83e152fe1e62",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "6086462a12d54bafa59d3c4566f06cb2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "74a6ba0c3cbc4051be0a83e152fe1e62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7d3f3d9e15894d05a4d188ff4f466554": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b40bdfb1ac1d4cffb7cefcb870c64d45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f1355871cc6f4dd4b50d9df5af20e5c8",
       "placeholder": "​",
       "style": "IPY_MODEL_ca245376fd9f4354af6b2befe4af4466",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00, 44.69it/s]"
      }
     },
     "ca245376fd9f4354af6b2befe4af4466": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dc83c7bff2f241309537a8119dfc7555": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e4ae2b6f5a974fd4bafb6abb9d12ff26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6086462a12d54bafa59d3c4566f06cb2",
       "placeholder": "​",
       "style": "IPY_MODEL_7d3f3d9e15894d05a4d188ff4f466554",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "f1355871cc6f4dd4b50d9df5af20e5c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
